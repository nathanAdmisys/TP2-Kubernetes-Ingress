# **TP Kubernetes Ingress**
# 

1. Installer Kind et crÃ©er votre premier cluster

```
curl -Lo ./kind https://kind.sigs.k8s.io/dl/v0.17.0/kind-linux-amd64
chmod +x ./kind
sudo mv ./kind /usr/local/bin/kind
```
- Pour crÃ©er le cluster, voici la commande :

```
kind create cluster --name cluster-kind1
```

2. Installer le Nginx ingress Controller

**Partie 1 :**

![](https://i.imgur.com/Qw5HBRh.png)

```
Creating cluster "kind" ...
 âœ“ Ensuring node image (kindest/node:v1.25.3) ğŸ–¼
 âœ“ Preparing nodes ğŸ“¦  
 âœ“ Writing configuration ğŸ“œ 
 âœ“ Starting control-plane ğŸ•¹ï¸ 
 âœ“ Installing CNI ğŸ”Œ 
 âœ“ Installing StorageClass ğŸ’¾ 
Set kubectl context to "kind-kind"
You can now use your cluster with:

kubectl cluster-info --context kind-kind
```

**Partie 2 :**

```
Root@Nathan:~/tp2$ kubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/main/deploy/static/provider/kind/deploy.yaml
namespace/ingress-nginx created
serviceaccount/ingress-nginx created
serviceaccount/ingress-nginx-admission created
role.rbac.authorization.k8s.io/ingress-nginx created
role.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrole.rbac.authorization.k8s.io/ingress-nginx created
clusterrole.rbac.authorization.k8s.io/ingress-nginx-admission created
rolebinding.rbac.authorization.k8s.io/ingress-nginx created
rolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx created
clusterrolebinding.rbac.authorization.k8s.io/ingress-nginx-admission created
configmap/ingress-nginx-controller created
service/ingress-nginx-controller created
service/ingress-nginx-controller-admission created
deployment.apps/ingress-nginx-controller created
job.batch/ingress-nginx-admission-create created
job.batch/ingress-nginx-admission-patch created
ingressclass.networking.k8s.io/nginx created
validatingwebhookconfiguration.admissionregistration.k8s.io/ingress-nginx-admission created
```


3. ComplÃ©ter le schÃ©ma suivant avec des objets Kubernetes

![](https://i.imgur.com/kILNAXL.png)


4. Builder et publier (Ã  partir de lâ€™image nginx) sur le DockerHub, une image docker pour chacun des sites web prÃ©sent sur le schÃ©ma prÃ©cÃ©dent. Vous devez avoir 3 images (une par magasin tacos, pizzas et burgers)

- Build des images

```
docker build -t pizza -f Dockerfile .
docker build -t burger -f Dockerfile .
docker build -t tacos -f Dockerfile .
```
- Tag des images

```
Root@Nathan:~/tp2/config-dockerfile$ docker tag pizza nathanynov/restaurants:pizza
Root@Nathan:~/tp2/config-dockerfile$ docker tag burger nathanynov/restaurants:burger
Root@Nathan:~/tp2/config-dockerfile$ docker tag tacos nathanynov/restaurants:tacos
```

- Push des images

```
docker push nathanynov/restaurants:pizza 
docker push nathanynov/restaurants:burger
docker push nathanynov/restaurants:tacos
```

5. Ecrire les fichiers yaml vous permettant de dÃ©ployer sur votre cluster kind installÃ© en local les composants dÃ©crits sur le schÃ©ma de la question 3 et les images crÃ©es Ã  la question 4

- Arborescence update avec le fichiers ingress.yml et conf.yml pour chaque restaurants : 

```
â””â”€â”€ tp2
    â”œâ”€â”€ pizza
    â”‚   â”œâ”€â”€ ConfPizza.yml
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â””â”€â”€ index.html
    â”œâ”€â”€ ingress.yml
    â”œâ”€â”€ burger
    â”‚   â”œâ”€â”€ ConfBurger.yml
    â”‚   â”œâ”€â”€ Dockerfile
    â”‚   â””â”€â”€ index.html
    â””â”€â”€ tacos
        â”œâ”€â”€ ConfTacos.yml
        â”œâ”€â”€ Dockerfile
        â””â”€â”€ index.html
```

Fichier ingress.yml

```
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ingress-resource-backend-restaurants
  annotations: 
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - host: mypizza.eatsout.com
    http:
      paths:
        - pathType: Prefix
          path: "/"
          backend:
            service:
              name: nginx-pizza-web-service
              port:
                number: 80
  - host: burgerandtacos.eatsout.com
    http:
      paths:
        - pathType: Prefix
          path: "/burgers"
          backend:
            service:
              name: nginx-burger-web-service
              port:
                number: 80
        - pathType: Prefix
          path: "/tacos"
          backend:
            service:
              name: nginx-tacos-web-service
              port:
                number: 80
```

Fichier ConfPizza.yml

```
apiVersion: v1
kind: Service
metadata:
  name: nginx-pizza-web-service
spec:
  selector:
    app: nginx-web-pizza
  ports:
  - name: http
    port: 80
    targetPort: 80
  type: ClusterIP
  
---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-pizza-web-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-web-pizza
  template:
    metadata:
      labels:
        app: nginx-web-pizza
    spec:
      containers:
      - name: cont-pizza
        image: nathanynov/restaurants:pizza
        ports:
        - containerPort: 80
```

Fichier ConfBurger.yml

```
apiVersion: v1
kind: Service
metadata:
  name: nginx-burger-web-service
spec:
  selector:
    app: nginx-web-burger
  ports:
  - name: http
    port: 80
    targetPort: 80
  type: ClusterIP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-burger-web-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-web-burger
  template:
    metadata:
      labels:
        app: nginx-web-burger
    spec:
      containers:
      - name: cont-burger
        image: nathanynov/restaurants:burger
        ports:
        - containerPort: 80
```

Fichier ConfTacos.yml

```
apiVersion: v1
kind: Service
metadata:
  name: nginx-tacos-web-service
spec:
  selector:
    app: nginx-web-tacos
  ports:
  - name: http
    port: 80
    targetPort: 80
  type: ClusterIP

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: nginx-tacos-web-deployment
spec:
  replicas: 1
  selector:
    matchLabels:
      app: nginx-web-tacos
  template:
    metadata:
      labels:
        app: nginx-web-tacos
    spec:
      containers:
      - name: cont-tacos
        image: nathanynov/restaurants:tacos
        ports:
        - containerPort: 80
```


6. Votre magasin de tacos devient trÃ¨s populaire (il va avoir 3 fois plus de commandes). Il va vous falloir gÃ©rer une charge importante sur le Service de commande des tacos. Comment gÃ©rez-vous cela ? Comment vÃ©rifier que la charge est bien rÃ©partie (avec quelle commande kubectl ?) ?


Pour vÃ©rifier que la charge soit bien rÃ©partie, on peut utiliser la commande â€œkubectl top podâ€.
Elle permet de donner des informations sur lâ€™utilisation des ressources sur les pods.

